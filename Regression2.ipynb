{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945a91e8-07b9-4926-96ef-d0ef53e5dcdb",
   "metadata": {},
   "source": [
    "# Q1.\n",
    "### Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f0fd4-b546-4059-9a61-eae9d85cd85a",
   "metadata": {},
   "source": [
    "R-squared is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a linear regression model.\n",
    "\n",
    "R^2 = 1 - SS(res)/SS(tot)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae543f-ee2b-4484-b9c0-a835ff6113f9",
   "metadata": {},
   "source": [
    "# Q2.\n",
    "### Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c01a20-5ef8-4c1e-acac-1cdf758d9967",
   "metadata": {},
   "source": [
    "Adjusted R-squared takes into account the number of predictors in the model, adjusting for the potential inclusion of irrelevant predictors.\n",
    "\n",
    "Adjusted R^2 = 1 - (1-R^2)*(n-1)/n-p-1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b7b64-a7b3-4aa7-b785-3d5fe45dc9ca",
   "metadata": {},
   "source": [
    "# Q3.\n",
    "### When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae390d77-b4d9-4d54-ab43-5284c187228d",
   "metadata": {},
   "source": [
    "- when comparing models with different numbers of predictors.\n",
    "- It helps to prevent overfitting by penalizing models that include unnecessary variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d7a3f-d04e-4d4a-8458-36edd26c279d",
   "metadata": {},
   "source": [
    "# Q4.\n",
    "### What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e859b-a287-40f3-99ae-dc6b5c0139b9",
   "metadata": {},
   "source": [
    "- RMSE, MSE , and MAE are metrics used to evaluate the performance of regression models.\n",
    "- RMSE is the square root of the average of squared differences between predicted and actual values.\n",
    "- MSE is the average of squared differences between predicted and actual values.\n",
    "- MAE is the average of absolute differences between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a22c51b-57a0-4ce0-91ad-546560aded04",
   "metadata": {},
   "source": [
    "# Q5.\n",
    "### Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c843e-b594-47f9-ad0c-f266fffd810a",
   "metadata": {},
   "source": [
    "- RMSE :\n",
    "    \n",
    "    - Advantages : \n",
    "        - Sensitive to large errors due to the square term, penalizing outliers more.\n",
    "        - Provides a measure of the spread of errors, giving a sense of how well the model is performing across the entire dataset.\n",
    "        \n",
    "    - Disadvantages:\n",
    "        - Sensitive to outliers, as the square amplifies their impact.\n",
    "        - Not as interpretable as other metrics due to the square root.\n",
    "        \n",
    "- MSE\n",
    "\n",
    "    - Advantages : \n",
    "        - Similar to RMSE but lacks the square root, making it computationally simpler.\n",
    "        - Also sensitive to large errors.\n",
    "        \n",
    "    - Disadvantages : \n",
    "        - Same as RMSE, sensitive to outliers.\n",
    "        - The lack of square root makes it harder to interpret in the original           units of the target variable.\n",
    "        \n",
    "- MAE\n",
    "\n",
    "    - Advantages : \n",
    "        - Not sensitive to outliers as it doesn't use a square term.\n",
    "        - Provides a more interpretable result in the original units.\n",
    "        \n",
    "    - Disadvantages : \n",
    "        - Ignores the direction of errors.\n",
    "        - May not penalize large errors enough in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3060c61-efb1-4c85-83fd-ea2831045127",
   "metadata": {},
   "source": [
    "# Q6.\n",
    "### Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d2ebf-3933-427a-a247-257f0aa2df10",
   "metadata": {},
   "source": [
    "- Lasso regularization is a linear regression technique that adds a penalty term to the cost function, which is the absolute value of the magnitude of the coefficients multiplied by a regularization parameter. The objective is to minimize the sum of squared residuals plus the sum of the absolute values of the coefficients.\n",
    "\n",
    "Difference : \n",
    "\n",
    "- Lasso uses the absolute values of the coefficients, promoting sparsity by pushing some coefficients to exactly zero.\n",
    "\n",
    "Appropriate : \n",
    "\n",
    "- When there's a belief that many features are irrelevant or contribute little to the prediction.\n",
    "- When feature selection is important, and you want a sparse model with fewer non-zero coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae788c53-f0d0-41ef-85b6-e46bb9d1e639",
   "metadata": {},
   "source": [
    "# Q7.\n",
    "### How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b685f5-ef91-4904-afb1-e04bab754027",
   "metadata": {},
   "source": [
    "- Regularized linear models add penalty terms to the cost function, discouraging the model from fitting the training data too closely. This helps prevent overfitting, especially when dealing with high-dimensional datasets where the number of features is close to or exceeds the number of samples.\n",
    "\n",
    "Example : Consider a dataset with many features, some of which may not contribute significantly to the target variable. Without regularization, the model might assign weights to all features, potentially capturing noise in the training data. Regularization constrains the model, leading to more generalizable and less overfitted solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb5a79-ee5f-45bf-8be2-38b360139c54",
   "metadata": {},
   "source": [
    "# Q8.\n",
    "### Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21818bc-67b6-4bb8-b4f8-0111a858992f",
   "metadata": {},
   "source": [
    "- Assumption of Linearity: Regularized linear models assume a linear relationship between features and target variable, which may not hold in all cases.\n",
    "- Selection of Regularization Parameter: The choice of the regularization parameter is crucial, and it may not be easy to determine the optimal value.\n",
    "- Interpretability: Adding penalty terms makes the interpretation of coefficients more challenging compared to non-regularized linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619772c-a9a0-4714-a13b-1cf702af9ba4",
   "metadata": {},
   "source": [
    "# Q9.\n",
    "### You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170df0b-0d69-4620-afab-6afc6e34de4b",
   "metadata": {},
   "source": [
    "- If small errors are acceptable, and you want to penalize larger errors more severely, you might prefer RMSE.\n",
    "- If you want a metric less sensitive to outliers and large errors, MAE might be more appropriate.\n",
    "\n",
    "Limitations : \n",
    "\n",
    "- The choice of metric depends on the specific context, and one metric may not capture all aspects of model performance.\n",
    "- Domain knowledge and the specific goals of the analysis should guide the choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f89a14-d5a8-480b-acf1-10abd5269f75",
   "metadata": {},
   "source": [
    "# Q10.\n",
    "### You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9472c-fd76-4ec6-b6af-18f3633b5f7f",
   "metadata": {},
   "source": [
    "Ridge Regularization (Model A):\n",
    "\n",
    "Uses the squared values of coefficients.\n",
    "Suitable when all features are expected to contribute, but some may have smaller effects.\n",
    "Lasso Regularization (Model B):\n",
    "\n",
    "Uses the absolute values of coefficients.\n",
    "Suitable when feature selection is essential, and some features can be eliminated without significantly affecting performance.\n",
    "Trade-offs and Limitations:\n",
    "\n",
    "Ridge tends to shrink coefficients towards zero but rarely sets them exactly to zero.\n",
    "Lasso can lead to sparsity by setting some coefficients exactly to zero.\n",
    "The choice depends on the balance between the need for feature selection and the desire for a more continuous shrinkage of coefficients.\n",
    "The performance of the models also depends on the specific characteristics of the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
